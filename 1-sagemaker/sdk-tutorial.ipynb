{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Introduction to Amazon SageMaker AI</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Overview](#Overview)\n",
    "2. [Amazon S3](#Amazon-S3)\n",
    "   * [Introduction](#Introduction)\n",
    "   * [Creating bucket](#Creating-bucket)\n",
    "   * [Uploading data](#Uploading-data)\n",
    "3. [Amazon SageMaker AI](#Amazon-SageMaker-AI)\n",
    "   * [Processing jobs](#Processing-jobs)\n",
    "   * [Training jobs](#Training-jobs)\n",
    "   * [Endpoints](#Endpoints)\n",
    "   * [Batch transform jobs](#Batch-transform-jobs)\n",
    "4. [CloudWatch Logs](#CloudWatch-Logs)\n",
    "5. [Epilogue](#Epilogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Overview\n",
    "Amazon SageMaker AI is an umbrella of services that AWS provides for Machine Learning (ML). In a nutshell, it is a service that enables the developer to be much more efficient with their valuable time when developing and deploying ML models. This methodology is applicable across many learning algorithms and many production use cases.\n",
    "\n",
    "In this tutorial, you will harness some of the most commonly used microservices of SageMaker AI to contruct basic components of a machine learning workflow. By the end of this lesson, you will be able to:\n",
    "* Launch a processing job to preprocess your data.\n",
    "* Launch a training job and build your ML model.\n",
    "* Deploy an endpoint to serve as an API for your trained model.\n",
    "* Launch a batch transform job to try out your trained model.\n",
    "<center><img src=\"img/sagemaker_microservices.png\" width=\"80%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon S3\n",
    "First of all, you need to create a bucket in Amazon S3 to store any future files and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Simple Storage Service (Amazon S3) is an object storage service that can store almost any object needed for machine learning. That includes datasets, model artifacts, logs, and more.\n",
    "\n",
    "An [S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html) is a container for objects (i.e., files) stored in S3. \n",
    "\n",
    "S3 supports the folder concept as a means of grouping objects. It does this by using a shared name *prefix*. In other words, the grouped objects have names that begin with a common string. This common string, or shared prefix, is the folder name. The prefix must end with a forward slash character `/` to indicate folder structure. Furthermore, object names are also referred to as key names.\n",
    "\n",
    "For example: `s3://example-bucket/1/2/3/example.txt`\n",
    " * Bucket: `example-bucket`\n",
    " * Prefix: `1/2/3/`\n",
    " * Key name: `1/2/3/example.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is importing `boto3` module, which is the AWS SDK for Python. You are encouraged to explore [its documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) for future practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then connect to Amazon S3 and create a bucket. Enter a name for your bucket in the code below. It must be globally unique accross all AWS accounts. Once created, you cannot change its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a service client to access S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'ml-workflow-2'  # Replace with a globally unique name for your bucket\n",
    "s3.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created a bucket on AWS, you can upload any files into this storage through `boto3` or AWS console. Furthermore, you can create folders to help organize your files more effectively. Simply run the code below and feel free to change the name of folders. You can also create nested folders by including more slashes, such as `parent-folder/child-folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix = 'data'  # Folder for datasets\n",
    "model_prefix = 'models'  #Folder for models\n",
    "\n",
    "s3.put_object(Bucket=bucket_name, Key=data_prefix)\n",
    "s3.put_object(Bucket=bucket_name, Key=model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating bucket, you can upload data and any other files there. But right now, you need to upload all datasets in [data folder](../data/) first using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_s3(file_name, s3_prefix=''):\n",
    "    key_name = s3_prefix + file_name.rsplit('/', maxsplit=1)[-1]\n",
    "    try:\n",
    "        s3.upload_file(file_name, bucket_name, key_name)\n",
    "    except ClientError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_to_s3('../data/reviews_Musical_Instruments_5.json.zip', data_prefix)\n",
    "upload_file_to_s3('../data/reviews_Patio_Lawn_and_Garden_5.json.zip', data_prefix)\n",
    "upload_file_to_s3('../data/reviews_Toys_and_Games_5.json.zip', data_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done successfully, you can see these datasets in your bucket from [S3 console](https://console.aws.amazon.com/s3/home)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading necessary files, the next thing to do is training a machine learning model and making use of it to produce inferences. This is the most important part as you will perform common machine learning operations on AWS.\n",
    "\n",
    "Step by step, you will create a model that predicts the usefulness of a product review, given only the text. This is an example of a problem in the domain of supervised sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training a model, you need input data. The [dataset](../data/reviews_Toys_and_Games_5.json.zip) you will be working with is a collection of reviews for an assortment of toys and games found on Amazon. It includes, but is not limited to, the text of the review itself as well as the number of user votes on whether or not the review was helpful.\n",
    "\n",
    "However, the dataset is inside a .zip file so you have to extract it before proceeding. Moreover, the dataset is a file containing a single JSON object per line representing a review with the following format: \n",
    "```JSON\n",
    "{\n",
    " \"reviewerID\": \"<string>\",\n",
    " \"asin\": \"<string>\",\n",
    " \"reviewerName\": \"<string>\",\n",
    " \"helpful\": [\n",
    "   <int>, (indicating number of helpful votes)\n",
    "   <int>  (indicating total number of votes)\n",
    " ],\n",
    " \"reviewText\": \"<string>\",\n",
    " \"overall\": <int>,\n",
    " \"summary\": \"<string>\",\n",
    " \"unixReviewTime\": <int>,\n",
    " \"reviewTime\": \"<string>\"\n",
    "}\n",
    "```\n",
    "Later, you will be using [BlazingText algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html) in training process, which is an implemention of [Word2Vec algorithm](https://en.wikipedia.org/wiki/Word2vec) optimized for SageMaker AI. Therefore, in order for this algorithm to work, you have to format the input data correctly. This is true for any other algorithm or model you work with, as each of them requires a particular type and structure of the input data. In this case, the data should only consist of plain text, with each line containing a label name followed by a sentence. Labels must be prefixed by the string `__label__`. \n",
    "\n",
    "For the dataset in this exercise, you will extract the text from the field *reviewText* and generate label based on the field *helpful* for each review. If the majority of votes is helpful, assign it `__label__1`, otherwise `__label__2`. If there is no majority or the review text is empty, drop the review from consideration. Then, cut the text into individual sentences, while ensuring that each sentence retains the original label from the review. When splitting using the character `.`, make sure that no empty sentences are created, since reviews usually contain an ellipsis `...` or more. Your input data should look something like this:\n",
    "\n",
    "```\n",
    "__label__1 Even if you can only play with one other person, you'll want to pull Stone Age out often\n",
    "__label__1 But if you have friends to join you, this game will be on the table a lot\n",
    "__label__2 It's a fun game but not a favorite\n",
    "__label__2 I prefer more complex games\n",
    "__label__2 If you're new to gaming or like relatively simple games I recommend you try this\n",
    "```\n",
    "\n",
    "Finally, it is your responsibility is to split the dataset into training set and testing set. Training set should represent 80% of the dataset, while the rest is testing set. Make sure that they don't overlap.\n",
    "\n",
    "All of the procedures mentioned above and more are collectively called *data pre-processing*, the first and most crucial step in any machine learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement a Python script to unzip, format, and split the raw dataset as previously instructed and save it in the same folder as this notebook. Or you can go ahead and use [hello_blaze_preprocess.py](hello_blaze_preprocess.py) provided.\n",
    "\n",
    "If you decide to custom your own script, note that the processing job will copy the dataset from S3 to a local directory, prefixed with `/opt/ml/processing/`, within the container. Thus, your script should take the dataset in this directory as input instead. Additionally, it must output training set and testing set to specified local directories, also prefixed with `/opt/ml/processing/`. You will have a chance to set up these local directories in the next steps.\n",
    "<center><img src=\"img/processing_model.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the upcoming steps, you are going to use `sagemaker` module, which is a higher-level AWS SDK specifically designed for Amazon SageMaker AI. Although `boto3` gives you general access to all AWS services, this module is specialized for tasks within the Amazon SageMaker AI microservices.\n",
    "\n",
    "You are also going to use the role that was created for this notebook instance to run SageMaker AI microservices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Get the execution role\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_prefix = '/opt/ml/processing/'\n",
    "preprocess_code = 'hello_blaze_preprocess.py'  # Replace with your own Python script\n",
    "\n",
    "# S3 path of the unprocessed dataset\n",
    "s3_dataset = 's3://' + bucket_name + data_prefix + 'reviews_Toys_and_Games_5.json.zip'\n",
    "\n",
    "# local directory path that the dataset will be downloaded into\n",
    "input_path = container_prefix + 'input'\n",
    "# local directory paths where your Python script saves the training/testing set\n",
    "train_path = container_prefix + 'output/train'\n",
    "test_path = container_prefix + 'output/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_path(filename='', prefix=''):\n",
    "    return 's3://' + bucket_name + prefix + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SKLearnProcessor, version 0.20.0\n",
    "sklearn_processor = SKLearnProcessor(role=role, framework_version='0.20.0', instance_type='ml.m5.large', instance_count=1)\n",
    "\n",
    "# Start a run job. You will pass in as parameters the local location of the processing code, \n",
    "# a processing input object, two processing output objects. The paths that you pass in here are directories, \n",
    "# not the files themselves. Check the preprocessing code for a hint about what these directories should be. \n",
    "\n",
    "# local directory path that the dataset will be downloaded into\n",
    "input_path = container_prefix + 'input'\n",
    "# local directory paths where your Python script saves the training/testing set\n",
    "train_path = container_prefix + 'output/train'\n",
    "test_path = container_prefix + 'output/test'\n",
    "\n",
    "sklearn_processor.run(code=preprocess_code,\n",
    "                      inputs=[ProcessingInput(\n",
    "                          source = 's3://udacity-ml-workflow/Lesson 2, Exercise 4/Toys_and_Games_5.json.zip', # the S3 path of the unprocessed data\n",
    "                          destination=input_path, \n",
    "                      )],\n",
    "                      outputs=[ProcessingOutput(source=train_path),# a 'local' directory path for where you expect the output for train data to be\n",
    "                               ProcessingOutput(source=test_path)]) # a 'local' directory path for where you expect the output for test data to be "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
